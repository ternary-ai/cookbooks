{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0b0826",
   "metadata": {},
   "source": [
    "# Named Entity Annotation with BERT Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8816a0",
   "metadata": {},
   "source": [
    "The cookbook loads a datasets of social media posts and news articles in JSON form and then applies BERT based named entity recognition to the texts. Based on the URL of the original post, the script also retrieves the full body from the website for parsing with BS4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d273579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade transformers beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bf49e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Huggingface data\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import LukeForEntityClassification\n",
    "from transformers import pipeline\n",
    "import json, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b49e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augentation\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import Request\n",
    "from urllib.error import HTTPError, URLError\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7b1b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script supports append mode in case the file is long. \n",
    "# For testing it is recommended to use a sample\n",
    "append_mode=False\n",
    "sample_only=True\n",
    "sample_size=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b006d9f",
   "metadata": {},
   "source": [
    "### Data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "742fb052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT file \n",
    "input_file=\"ner_pre_dataset20230925.json\"\n",
    "\n",
    "#OUTPUT file - A result looks like this https://github.com/ternary-ai/datasets/blob/main/ner_sentences_2.5M_en_sample10K.csv\n",
    "output_file = 'ner_annotated_2.5M.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b9470e1",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#BS4 page parser functions\n",
    "def get_page(url):\n",
    "    \"\"\"Scrapes a URL and returns the HTML source.\n",
    "\n",
    "    Args:\n",
    "        url (string): Fully qualified URL of a page.\n",
    "\n",
    "    Returns:\n",
    "        soup (string): HTML source of scraped page.\n",
    "    \"\"\"\n",
    "    #print(url)\n",
    "    #return\n",
    "    req = Request(f\"{json.loads(url)}\", headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    soup=None\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req, timeout=10).read().decode('utf-8')\n",
    "    except HTTPError as error:\n",
    "        print('Data not retrieved because %s\\nURL: %s', error, url)\n",
    "        return False\n",
    "    except URLError as error:\n",
    "        if isinstance(error.reason, socket.timeout):\n",
    "            print('socket timed out - URL %s', url)\n",
    "            return False\n",
    "        else:\n",
    "            print('some other error happened %s ' % error)\n",
    "            return False\n",
    "    else:\n",
    "        \n",
    "        try:\n",
    "            soup = BeautifulSoup(response,\n",
    "                         'html.parser',\n",
    "                         from_encoding=response.info().get_param('charset'))\n",
    "        except:\n",
    "            soup = BeautifulSoup(response,\n",
    "                                 'html.parser')\n",
    "            return False\n",
    "        \n",
    "        element = soup.find('body')\n",
    "\n",
    "        text_content = element.get_text(' | ',strip=True)\n",
    "\n",
    "    return text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91f319e9",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def write_json(new_data, filename='data.json'):\n",
    "\n",
    "    with open(filename,'r+') as file:\n",
    "          # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data.append(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        #json.dump(file_data, file, indent = 4)\n",
    "        json.dump(file_data, file, indent = 4, default=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbd07410",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7bf0c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data has 257648 observations\n"
     ]
    }
   ],
   "source": [
    "# Print the loaded data\n",
    "print(f\" data has {len(data)} observations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4baa6",
   "metadata": {},
   "source": [
    "### Set random index and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df29623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa8b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf216120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Models    \n",
    "model_name=\"dslim/bert-large-NER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88d59f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac4126eebd14de59a97fd2714d4a551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  84%|########4 | 1.12G/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Load AutoTokenizer this might take a while\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df34a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer,device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aedef21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a randome sample from the data set\n",
    "full_data=[]\n",
    "if sample_only:\n",
    "    tdata=data[random_index:random_index+sample_size]\n",
    "else:\n",
    "    tdata=data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04debe29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2c3ed54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing 5/5 => 100.0% complete\r"
     ]
    }
   ],
   "source": [
    "#Now execute the loop given the dataset\n",
    "for index, entry in enumerate(tdata):\n",
    "    \n",
    "    print(f\" processing {index+1}/{len(tdata)} => {round(((index+1)/len(tdata)*100),5)}% complete\", end='\\r', flush=True)\n",
    "    source_value_lower = entry['_source'].lower()\n",
    "\n",
    "    if 'youtu' in source_value_lower or 'redd.it' in source_value_lower or 'reddit' in source_value_lower or 'twitter' in source_value_lower or 'finclout' in source_value_lower:\n",
    "        #print(\" **** processing social **** \\n\")\n",
    "        _content= entry['_content']\n",
    "    else:\n",
    "        try:\n",
    "            cTmp=get_page(entry['_url'])\n",
    "            if cTmp:\n",
    "                _content= cTmp\n",
    "            else:\n",
    "                _content= entry['_content']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"***** BS4 failed for URL: **** => {entry['_url']} *****\")\n",
    "            _content= entry['_content']\n",
    "\n",
    "    \n",
    "    #Here is the NER model appending the identified entities\n",
    "    ner_results = nlp(entry['_title'])\n",
    "    entry.update({\"_tEntities\":ner_results})\n",
    "\n",
    "    ner_results = nlp(_content)\n",
    "    entry.update({\"_cEntities\":ner_results})\n",
    "    entry.update({\"_expContent\":_content})\n",
    "\n",
    "    full_data.append(entry)\n",
    "\n",
    "    if append_mode:\n",
    "        write_json(entry,output_file)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3b6f2",
   "metadata": {},
   "source": [
    "{\"entity\": \"I-PER\", \"score\": \"0.98287416\", \"index\": 6, \"word\": \"##asa\", \"start\": 11, \"end\": 14}, \n",
    "{\"entity\": \"I-PER\", \"score\": \"0.971494\", \"index\": 7, \"word\": \"##nt\", \"start\": 14, \"end\": 16}, \n",
    "{\"entity\": \"I-PER\", \"score\": \"0.9994259\", \"index\": 8, \"word\": \"P\", \"start\": 17, \"end\": 18}, \n",
    "{\"entity\": \"I-PER\", \"score\": \"0.9972548\", \"index\": 9, \"word\": \"##rab\", \"start\": 18, \"end\": 21}, {\"entity\": \"I-PER\", \"score\": \"0.99404114\", \"index\": 10, \"word\": \"##hu\", \"start\": 21, \"end\": 23}, {\"entity\": \"B-PER\", \"score\": \"0.8141747\", \"index\": 31, \"word\": \"Ali\", \"start\": 126, \"end\": 129}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2793adb",
   "metadata": {},
   "source": [
    "### Now store the dataset in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "313b4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the array of dictionaries to a JSON file\n",
    "if not append_mode:\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(full_data, json_file, default=str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
